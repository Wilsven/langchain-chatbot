{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Redis LangChain OpenAI eCommerce Chatbot](https://redis.com/blog/build-ecommerce-chatbot-with-redis/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch and prepare the products dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Dataset\n",
    "\n",
    "We will be working with the [Amazon Berkeley Objects](https://amazon-berkeley-objects.s3.amazonaws.com/index.html) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gdown 1tHWB6u3yQCuAgOYc-DxtZ8Mru3uV5_lj"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Dataset\n",
    "\n",
    "We truncate the longer text fields. That’s to keep our dataset a bit leaner, which saves on memory and compute time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "MAX_TEXT_LENGTH = 512  # Maximum num of text characters to use\n",
    "\n",
    "\n",
    "def auto_truncate(text: str) -> str:\n",
    "    \"\"\"Truncate the given text.\"\"\"\n",
    "    return text[:MAX_TEXT_LENGTH]\n",
    "\n",
    "\n",
    "# Load Product data and truncate long text fields\n",
    "all_prods_df = pd.read_csv(\n",
    "    \"product_data.csv\",\n",
    "    converters={\n",
    "        \"bullet_point\": auto_truncate,\n",
    "        \"item_keywords\": auto_truncate,\n",
    "        \"item_name\": auto_truncate,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(all_prods_df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform some final preprocessing steps to construct a primary key, clean up the keywords field and to drop missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contruct a primary key from item ID and domain name\n",
    "all_prods_df[\"primary_key\"] = (\n",
    "    all_prods_df[\"item_id\"] + \"-\" + all_prods_df[\"domain_name\"]\n",
    ")\n",
    "# Replace empty strings with None and drop\n",
    "all_prods_df[\"item_keywords\"].replace(\"\", None, inplace=True)\n",
    "all_prods_df.dropna(subset=[\"item_keywords\"], inplace=True)\n",
    "\n",
    "# Reset pandas dataframe index\n",
    "all_prods_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "all_prods_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full dataset contains over 100,000 products, but we will restrict it to a subset of 2,500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number products to use (subset)\n",
    "NUMBER_PRODUCTS = 2500\n",
    "\n",
    "# Get the first 1000 products with non-empty item keywords\n",
    "product_metadata = all_prods_df.head(NUMBER_PRODUCTS).to_dict(orient=\"index\")\n",
    "\n",
    "# Check one of the products\n",
    "product_metadata[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Redis as a Vector Database\n",
    "\n",
    "LangChain has a simple wrapper around [Redis](https://github.com/hwchase17/langchain/blob/master/langchain/vectorstores/redis.py) to help you load text data and to create embeddings that capture “meaning.” In this code, we prepare the product text and metadata, prepare the text embeddings provider (OpenAI), assign a name to the search index, and provide a Redis URL for connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores.redis import Redis as RedisVectorStore\n",
    "\n",
    "# loads .env file with your OPENAI_API_KEY\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data that will be embedded and converted to vectors\n",
    "texts = [v[\"item_name\"] for _, v in product_metadata.items()]\n",
    "\n",
    "# Product metadata that we'll store along our vectors\n",
    "metadatas = list(product_metadata.values())\n",
    "\n",
    "# Define embedding model\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Name of the Redis search index to create\n",
    "index_name = \"products\"\n",
    "\n",
    "# Assumes you have a redis stack server running on local host\n",
    "redis_url = \"redis://localhost:6379\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Redis vectorstore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and load redis with documents\n",
    "vectorstore = RedisVectorStore.from_texts(\n",
    "    texts=texts,\n",
    "    metadatas=metadatas,\n",
    "    embedding=embeddings,\n",
    "    index_name=index_name,\n",
    "    redis_url=redis_url,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the LangChain conversational chain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the ChatBot with ConversationalRetrieverChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.base import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redis holds our product catalog including metadata and OpenAI-generated embeddings that capture the semantic properties of the product content. Under the hood, using [Redis Vector Similarity Search](https://redis.io/docs/stack/search/reference/vectors/) (VSS), the chatbot queries the catalog for products that are most similar to or relevant to what the user is shopping for. No fancy keyword search or manual filtering is needed; VSS takes care of it.\n",
    "\n",
    "The `ConversationalRetrievalChain` that forms the chatbot operates in three phases:\n",
    "\n",
    "1. **Question creation** evaluates the input question and uses the OpenAI GPT model to combine it with knowledge from previous conversational interactions (if any).\n",
    "2. **Retrieval** searches Redis for the best available products, given the items in which the  shopper expressed interest.\n",
    "3. **Question answering** gets the product results from the vector search query and uses the OpenAI GPT model to help the shopper navigate the options."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Engineering\n",
    "\n",
    "Below is the prompt defined for steps 1 and 3 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Given the following chat history and a follow up question, rephrase the follow up input question to be a standalone question.\n",
    "Or end the conversation if it seems like it's done.\n",
    "\n",
    "Chat History:\\\"\"\"\n",
    "{chat_history}\n",
    "\\\"\"\"\n",
    "\n",
    "Follow Up Input: \\\"\"\"\n",
    "{question}\n",
    "\\\"\"\"\n",
    "\n",
    "Standalone question:\"\"\"\n",
    "\n",
    "condense_question_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "template = \"\"\"You are a friendly, conversational retail shopping assistant. Use the following context including product names, descriptions, and keywords to show the shopper whats available, help find what they want, and answer any questions.\n",
    "It's ok if you don't know the answer.\n",
    "\n",
    "Context:\\\"\"\"\n",
    "{context}\n",
    "\\\"\"\"\n",
    "\n",
    "Question:\\\"\n",
    "\\\"\"\"\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "qa_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define two OpenAI LLMs and wrap them with chains for question generation and question answering respectively. The `streaming_llm` allows us to pipe the chatbot responses to stdout, token by token, giving it a charming, chatbot-like user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two LLM models from OpenAI\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "streaming_llm = OpenAI(\n",
    "    streaming=True,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    verbose=True,\n",
    "    temperature=0.2,\n",
    "    max_tokens=150,\n",
    ")\n",
    "\n",
    "# Use the LLM Chain to create a question creation chain\n",
    "question_generator = LLMChain(llm=llm, prompt=condense_question_prompt)\n",
    "\n",
    "# Use the streaming LLM to create a question answering chain\n",
    "doc_chain = load_qa_chain(llm=streaming_llm, chain_type=\"stuff\", prompt=qa_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we tie it all together with the `ConversationalRetrievalChain` that wraps all three steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = ConversationalRetrievalChain(\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    combine_docs_chain=doc_chain,\n",
    "    question_generator=question_generator,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with the friendly virtual shopping assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chat history buffer\n",
    "chat_history = []\n",
    "\n",
    "# Gather user input for the first question to kick off the bot\n",
    "question = input(\"Hi! What are you looking for today?\")\n",
    "\n",
    "# Keep the bot running in a loop to simulate a conversation\n",
    "while True:\n",
    "    result = chatbot({\"question\": question, \"chat_history\": chat_history})\n",
    "    print(\"\\n\")\n",
    "    chat_history.append((result[\"question\"], result[\"answer\"]))\n",
    "    question = input()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize your chains for better performance\n",
    "\n",
    "We customize the `BaseRetriever` class to perform some document preprocessing before it returns the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain.schema import BaseRetriever\n",
    "from langchain.vectorstores import VectorStore\n",
    "from langchain.schema import Document\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class RedisProductRetriever(BaseRetriever, BaseModel):\n",
    "    vectorstore: VectorStore\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def combine_metadata(self, doc) -> str:\n",
    "        metadata = doc.metadata\n",
    "        return (\n",
    "            \"Item Name: \"\n",
    "            + metadata[\"item_name\"]\n",
    "            + \". \"\n",
    "            + \"Item Description: \"\n",
    "            + metadata[\"bullet_point\"]\n",
    "            + \". \"\n",
    "            + \"Item Keywords: \"\n",
    "            + metadata[\"item_keywords\"]\n",
    "            + \".\"\n",
    "        )\n",
    "\n",
    "    def get_relevant_documents(self, query):\n",
    "        docs = []\n",
    "        for doc in self.vectorstore.similarity_search(query):\n",
    "            content = self.combine_metadata(doc)\n",
    "            docs.append(Document(page_content=content, metadata=doc.metadata))\n",
    "        return docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup ChatBot with new retriever\n",
    "\n",
    "Update the retrieval class and chatbot to use the custom implementation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redis_product_retriever = RedisProductRetriever(vectorstore=vectorstore)\n",
    "\n",
    "chatbot = ConversationalRetrievalChain(\n",
    "    retriever=redis_product_retriever,\n",
    "    combine_docs_chain=doc_chain,\n",
    "    question_generator=question_generator,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chat history buffer\n",
    "chat_history = []\n",
    "\n",
    "# Gather user input for the first question to kick off the bot\n",
    "question = input(\"Hi! What are you looking for today?\")\n",
    "\n",
    "# Keep the bot running in a loop to simulate a conversation\n",
    "while True:\n",
    "    result = chatbot({\"question\": question, \"chat_history\": chat_history})\n",
    "    print(\"\\n\")\n",
    "    chat_history.append((result[\"question\"], result[\"answer\"]))\n",
    "    question = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
